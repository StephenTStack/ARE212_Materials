{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Weighted Regression in `python`                                   :jupyter:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The fact that $T$ and $u$ are &ldquo;independent&rdquo; (or at least\northogonal) variables means that if we want to compute a\n&ldquo;classical&rdquo; regression we&rsquo;d do it something like this:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Define independent random variables\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%matplotlib inline\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\nk = 3 # Number of observables in T\n\nmu = [0]*k\nSigma=[[1,0.5,0],\n       [0.5,2,0],\n       [0,0,3]]\n\nT = multivariate_normal(mu,Sigma)\n\nu = multivariate_normal(cov=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["##### Define `X`\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Recall that $X$ can depend on $T$ and $u$.  This dependence needn&rsquo;t be\nlinear!  For example, suppose $X=T^3D + u$, where $D$ is an\n$\\ell\\times k$ matrix.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Construct Sample\n\n"]},{"cell_type":"markdown","metadata":{},"source":["To construct a sample of observables $(y,X,T)$ we just use the regression equation,\n      plus an assumption about the value of $\\beta$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["beta = [1/2,1]\n\nD = np.random.random(size=(3,2)) # Generate random 3x2 matrix\n\nN=1000 # Sample size\n\n# Now: Transform rvs into a sample\nT = T.rvs(N)\n\nu = u.rvs(N) # Replace u with a sample\n\nX = (T**3)@D  # Note use of ** operator for exponentiation\n\ny = X@beta + u # Note use of @ operator for matrix multiplication"]},{"cell_type":"markdown","metadata":{},"source":["##### Turn to estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["So, we now have data on *realizations* $(y,X,T)$.  Now forget\n     that we know $\\beta$ and let&rsquo;s estimate it, using weighted least\n     squares.  As a numerical matter it&rsquo;s better to avoid explicitly\n     inverting the $(T^T X)$ matrix; instead we can solve the &ldquo;normal&rdquo;\n     equations\n\n\\begin{align*}\n   T'y &= T' X b + T' u\\\\\n   \\mbox{E}(T'u) = 0\n\\end{align*}\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Numerical solution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In the classical case we were trying to solve a linear system that\n took the form $Ab=0$, with $A$ a square matrix.  In the present case\n we&rsquo;re also trying to solve a linear system, but with a matrix $A$\n that may have more rows than columns.  Provided the rows are linearly\n independent, this implies that we have an **overidentified** system of\n equations.  We&rsquo;ll return to the implications of this later, but for\n now this also calls for a different numerical approach, using\n `np.linalg.lstsq` instead of `np.linalg.solve`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.linalg import inv, sqrtm\n\nb = np.linalg.lstsq(T.T@X,T.T@y,rcond=None)[0] # lstsq returns several results\n\ne = y - X@b\n\nprint(b)\n\nTXplus = np.linalg.pinv(T.T@X) # Moore-Penrose pseudo-inverse\n\n# Covariance matrix of b\nvb = e.var()*TXplus@T.T@T@TXplus.T  # u is known to be homoskedastic\n\nprint(vb)"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
